{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Information\n",
        "\n",
        "RAG VERSION 7.0\n",
        "\n",
        "--------------------\n",
        "\n",
        "Lĩnh vực : Luật doanh nghiệp Việt Nam\n",
        "\n",
        "Embedding : keepitreal/vietnamese-sbert\n",
        "\n",
        "Vector Database : Chroma\n",
        "\n",
        "LLM Model : gemini-2.0-flash-exp\n",
        "\n",
        "Input document : txt\n",
        "\n",
        "Update : Evaluation bằng llm\n",
        "\n",
        "Reranker Model : namdp-ptit/ViRanker\n",
        "\n",
        "Key word search : BM25\n"
      ],
      "metadata": {
        "id": "2QG3MQRU5YXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "Fpgkb3aoH0jC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U langchain-community\n",
        "!pip install -q chromadb\n",
        "!pip install -q unstructured\n",
        "!pip install -q sentence-transformers\n",
        "!pip install -q rank_bm25\n",
        "!pip install -q -U rouge_score\n",
        "!pip install -q streamlit\n",
        "!pip install -q pyngrok"
      ],
      "metadata": {
        "id": "SLUc382PdkXM",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QF2CRaq9bl7D",
        "outputId": "52068be5-0581-420c-e209-388e89d507f8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataLoader"
      ],
      "metadata": {
        "id": "HjovjuO2CbuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_loader.py\n",
        "import zipfile, os\n",
        "\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.retrievers import BM25Retriever\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "class DataLoader():\n",
        "    def __init__(self, source_path_1, source_path_2, chroma_path, chunk_size=1000, chunk_overlap=200):\n",
        "        self.source_path_1 = source_path_1\n",
        "        self.source_path_2 = source_path_2\n",
        "        self.choroma_path = chroma_path\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "\n",
        "    def load_data(self):\n",
        "      if not os.path.exists(self.source_path_2):\n",
        "        os.makedirs(self.source_path_2)\n",
        "\n",
        "\n",
        "      for path in self.source_path_1:\n",
        "\n",
        "        try:\n",
        "          with zipfile.ZipFile(path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(self.source_path_2)\n",
        "            print(f\"Successfully unzipped '{path}' to '{self.source_path_2}'\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: Zip file not found at '{path}'\")\n",
        "            pass\n",
        "        except zipfile.BadZipFile:\n",
        "            print(f\"Error: Invalid zip file at '{path}'\")\n",
        "            pass\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            pass\n",
        "\n",
        "\n",
        "      loader = DirectoryLoader(self.source_path_2, glob=\"*.txt\", show_progress=True)\n",
        "      documents = loader.load()\n",
        "\n",
        "      text_splitter = RecursiveCharacterTextSplitter(chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\n",
        "      chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "      bm25_retriever = BM25Retriever.from_documents(chunks)\n",
        "\n",
        "      os.makedirs(self.choroma_path, exist_ok=True)\n",
        "\n",
        "      os.system(f\"chmod -R u+w {self.choroma_path}\")\n",
        "\n",
        "      db = Chroma.from_documents(chunks, embedding = HuggingFaceEmbeddings(model_name=\"keepitreal/vietnamese-sbert\"),\\\n",
        "        persist_directory=self.choroma_path)\n",
        "\n",
        "      db.persist()\n",
        "\n",
        "      print(f\"Save {len(chunks)} chunks to {self.choroma_path}\")\n",
        "\n",
        "      return db, bm25_retriever\n"
      ],
      "metadata": {
        "id": "afrfquyFlfES",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc94d19c-758a-428c-8cc7-232663135e53"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting data_loader.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hybrid Search"
      ],
      "metadata": {
        "id": "hVULJxj4CfsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hybrid_search.py\n",
        "from langchain.retrievers import BM25Retriever,EnsembleRetriever\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "class HybridSearch:\n",
        "\n",
        "  def __init__(self, db, retriever):\n",
        "    self.db = db\n",
        "    self.retriever = retriever\n",
        "    self.ensemble_retriever = None\n",
        "\n",
        "\n",
        "  def setup(self):\n",
        "    chroma_retriever = self.db.as_retriever()\n",
        "    self.ensemble_retriever = EnsembleRetriever(retrievers=[self.retriever,chroma_retriever], weights=[0.3,0.7])\n",
        "\n",
        "\n",
        "  def keyword_search(self, query , top_k = 10):\n",
        "    self.retriever.k = top_k\n",
        "    results = self.retriever.invoke(query)\n",
        "    return results\n",
        "\n",
        "  def semantic_search(self, query, top_k = 10):\n",
        "    results = self.db.similarity_search_with_score(query, k = top_k)\n",
        "    return results\n",
        "\n",
        "  def hybrid_search(self, query, top_k = 10):\n",
        "\n",
        "    if(self.ensemble_retriever == None):\n",
        "      self.setup()\n",
        "\n",
        "    results = self.ensemble_retriever.invoke(query)\n",
        "    return results[:top_k]\n",
        "\n"
      ],
      "metadata": {
        "id": "Sk-67ADgKueL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04fa82d1-da71-4a41-da19-3a1d76720e99"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting hybrid_search.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reranker"
      ],
      "metadata": {
        "id": "RjfN1IwaCiOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reranker.py\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "class Reranker:\n",
        "  def __init__(self, model_name=\"namdp-ptit/ViRanker\"):\n",
        "    self.model = CrossEncoder(model_name)\n",
        "\n",
        "  def rerank(self, query, retrieved_docs, hybrid_search=True):\n",
        "    \"\"\"Rank documents based on relevance to the query.\"\"\"\n",
        "    # Combine query and document for reranking\n",
        "    if hybrid_search:\n",
        "      query_doc_pairs = [(query, doc.page_content) for doc in retrieved_docs]\n",
        "    else:\n",
        "      query_doc_pairs = [(query, doc[0].page_content) for doc in retrieved_docs]\n",
        "    scores = self.model.predict(query_doc_pairs)\n",
        "\n",
        "    # Sort documents by scores in descending order\n",
        "    ranked_results = sorted(zip(scores, retrieved_docs), key=lambda x: x[0], reverse=True)\n",
        "    return [doc for score, doc in ranked_results]"
      ],
      "metadata": {
        "id": "7QKh4eDgOZWL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5de937ba-4598-4523-93df-367bac5598f6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting reranker.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate functions"
      ],
      "metadata": {
        "id": "FFNwySV3DcwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile utils_function.py\n",
        "\n",
        "import google.generativeai as genai\n",
        "import json, os\n",
        "import config as cfg\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "def json2dict(response):\n",
        "    response_1 = response.text.replace(\"json\", \"\").replace(\"```\", \"\")\n",
        "\n",
        "    try:\n",
        "        response_dict = json.loads(response_1)\n",
        "        return response_dict\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON: {e}\")\n",
        "        print(f\"Problematic text: {response_1}\")\n",
        "\n",
        "\n",
        "\n",
        "def __generate_answer(prompt):\n",
        "    genai.configure(api_key=\"AIzaSyAj3NzXnmGGiSxzaN8BKkXh1kbEmtyJ8pc\")\n",
        "    model = genai.GenerativeModel(\"gemini-2.0-flash-exp\")\n",
        "    response = model.generate_content(prompt)\n",
        "    return response\n",
        "\n",
        "\n",
        "def classfication_article(question, template):\n",
        "\n",
        "  prompt_template = ChatPromptTemplate.from_template(template)\n",
        "  prompt = prompt_template.format(question=question)\n",
        "\n",
        "  return  __generate_answer(prompt).text\n",
        "\n",
        "\n",
        "def gen_evaluation_score(template, question, context, answer, ground_truth):\n",
        "\n",
        "  prompt_template = ChatPromptTemplate.from_template(template)\n",
        "  prompt = prompt_template.format(question=question, context=context, answer=answer, ground_truth=ground_truth)\n",
        "\n",
        "  return json2dict( __generate_answer(prompt))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def gen_hypothetical_document(question, chat_history=\"\"):\n",
        "    # Thêm lịch sử chat vào prompt\n",
        "    history_context = \"\"\n",
        "    if chat_history:\n",
        "        history_context = f\"Previous conversation:\\n{chat_history}\\n\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    {history_context}\n",
        "    ------------------------\n",
        "    Write a paragraph about 200 characters that answers this question, using Vietnamese only, if you can not answer, you just need to answer whatever you want.\n",
        "    Question: {question}\n",
        "    \"\"\"\n",
        "    return __generate_answer(prompt)\n",
        "\n",
        "def format_chat_history(messages):\n",
        "    formatted_history = \"\"\n",
        "    for msg in messages:\n",
        "        role = \"User\" if msg[\"role\"] == \"user\" else \"Assistant\"\n",
        "        formatted_history += f\"{role}: {msg['content']}\\n\"\n",
        "    return formatted_history\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def gen_ans(query, template, db, reranker, hybrid_search, chat_history=[]):\n",
        "\n",
        "    flag = 0\n",
        "\n",
        "    formatted_history = format_chat_history(chat_history)\n",
        "\n",
        "    articles = classfication_article(query, cfg.PROMPT_TEMPLATE_ARTICLE)\n",
        "\n",
        "\n",
        "    if \"Không\" not in articles:\n",
        "        flag = 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if flag == 1:\n",
        "\n",
        "        first_split = articles.split(\"[\")[1]\n",
        "        second_split = first_split.split(\"]\")[0]\n",
        "        second_split = second_split.replace('\"', '')\n",
        "\n",
        "        my_list = second_split.split(\",\")\n",
        "        my_list = [x.strip() for x in my_list]\n",
        "\n",
        "        content_list = []\n",
        "        source_law = []\n",
        "\n",
        "        law_list = [\"LDN_2020\", \"LDT_2020\", \"LLD_2019\", \"LCK_2019\", \"NQ173_2024\", \"ND135_2020\", \"ND145_2020\",]\n",
        "\n",
        "\n",
        "        for items in my_list:\n",
        "\n",
        "            for law in law_list:\n",
        "                try:\n",
        "\n",
        "                  path = f\"/content/books/{items}_{law}.txt\"\n",
        "\n",
        "                  if os.path.exists(path):\n",
        "                    with open(path, \"r\") as f:\n",
        "                        content = f.read()\n",
        "                        content_list.append(content)\n",
        "                        source_law.append(path)\n",
        "                except:\n",
        "                  pass\n",
        "\n",
        "        if(len(content_list) == 0):\n",
        "          flag = 0\n",
        "        else:\n",
        "          context_text = \"\\n\\n---\\n\\n\".join(content_list)\n",
        "\n",
        "    if flag == 0:\n",
        "\n",
        "\n",
        "        # print(\"1. Generating hyDE...\")\n",
        "        response = gen_hypothetical_document(query, formatted_history)\n",
        "        # print(\"1. Generated hyDE\")\n",
        "\n",
        "        # print(f\"HyDE text : {response.text}\\n\")\n",
        "\n",
        "        question = response.text\n",
        "\n",
        "\n",
        "        # question = query\n",
        "\n",
        "        # results = db.similarity_search_with_score(query, k = 10)\n",
        "\n",
        "        results = hybrid_search.hybrid_search(question, top_k=30)\n",
        "\n",
        "        results = reranker.rerank(question, results)[:10]\n",
        "\n",
        "        # results = hybrid_search.semantic_search(question, top_k=10)\n",
        "\n",
        "\n",
        "\n",
        "        source_law = []\n",
        "\n",
        "\n",
        "\n",
        "        for i in range(len(results)):\n",
        "          if results[i].metadata['source'] not in source_law:\n",
        "            source_law.append(results[i].metadata['source'])\n",
        "\n",
        "        # for i in range(len(results)):\n",
        "        #   if results[i][0].metadata['source'] not in source_law:\n",
        "        #     source_law.append(results[i][0].metadata['source'])\n",
        "\n",
        "\n",
        "        context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in results])\n",
        "        # context_text = \"\\n\\n---\\n\\n\".join([doc[0].page_content for doc in results])\n",
        "\n",
        "\n",
        "    prompt_template = ChatPromptTemplate.from_template(template)\n",
        "    prompt = prompt_template.format(context=context_text, question=query, history=formatted_history)\n",
        "\n",
        "    # print(\"2. Generating answer...\")\n",
        "    response =  __generate_answer(prompt)\n",
        "    # print(\"2. Generated answer\")\n",
        "\n",
        "\n",
        "    response_dict = json2dict(response)\n",
        "\n",
        "    details_law = \"Không có luật liên quan\"\n",
        "\n",
        "    try:\n",
        "        relevent_law = response_dict[\"relevent_law\"]\n",
        "\n",
        "        if(relevent_law == \"Không có luật liên quan\"):\n",
        "            return [response_dict[\"answer\"], \"Không có luật liên quan\", details_law, context_text]\n",
        "\n",
        "        deltails_law_list = []\n",
        "\n",
        "        for i in range(len(source_law)):\n",
        "            with open(source_law[i], \"r\") as f:\n",
        "                details_law = f.read()\n",
        "\n",
        "            my_str = \"\"\n",
        "            details_law_lines = details_law.splitlines()\n",
        "            for line in details_law_lines:\n",
        "                my_str += line + \"\\n\\n\"\n",
        "\n",
        "            path = source_law[i]\n",
        "            articles = path.replace(\"/content/books/\", \"\").replace(\".txt\", \"\")\n",
        "\n",
        "            articles_list = articles.split(\"_\")\n",
        "\n",
        "            articles_name = articles_list[0]\n",
        "\n",
        "\n",
        "            law_name = articles_list[1]\n",
        "\n",
        "            law_name = cfg.law_dict[law_name]\n",
        "\n",
        "\n",
        "            if \"ND\" in articles_list[1] or \"NQ\" in articles_list[1]:\n",
        "                my_articles = f\"{law_name} năm {articles_list[2]}\"\n",
        "            else:\n",
        "\n",
        "                my_articles = f\"{law_name} : Điều {articles_name}\"\n",
        "\n",
        "\n",
        "            deltails_law_list.append([my_articles, my_str])\n",
        "\n",
        "        return [response_dict[\"answer\"], response_dict[\"relevent_law\"], deltails_law_list, context_text]\n",
        "\n",
        "    except:\n",
        "\n",
        "        return [response_dict[\"answer\"], \"Không có luật liên quan\", details_law, context_text]\n",
        "\n",
        "\n",
        "def read_list_question(data_path):\n",
        "\n",
        "    question_list = []\n",
        "\n",
        "    with open(data_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            if len(line) != 0:\n",
        "                question_list.append(line)\n",
        "\n",
        "    return question_list"
      ],
      "metadata": {
        "id": "35DqbiRAfuPK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3926cbbf-5c04-4c83-b4e3-9e17c6fe850e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting utils_function.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SETUP"
      ],
      "metadata": {
        "id": "BzZecmZNvUhp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up Global Variable"
      ],
      "metadata": {
        "id": "M5OFHwX-Fs4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile config.py\n",
        "\n",
        "PROMPT_TEMPLATE =\\\n",
        "\"\"\"\n",
        "You are an assistant with the role of legal consultant in the field of Vietnamese Business Law, your task is to answer questions from users in the most complete and detailed way from the context in the \"`` `\" and question in the \"'''\", your answer should follow these requirement:\n",
        "1. you need to answer in Vietnamese only.\n",
        "2. answer in json format, first key is \"answer\", second key is \"relevent_law\".\n",
        "3. In \"relevent_law\" you need to indicate which law your \"answer\" belongs to in Vietnamese business law (including relevant articles and clauses).\n",
        "3. if you can not answer question, in \"answer\" key, answer \"Đã có lỗi xảy ra, vui lòng thử lại\".\n",
        "4. If you can not find any law in context, in \"relevent_law\", answer \"Không có luật liên quan\".\n",
        "5. You will also provided chat history from previous questions in \"###\"\n",
        "\n",
        "There is some sample answer:\n",
        "\n",
        "Question : \"Luật doanh nghiệp Việt Nam là gì?\"\n",
        "\n",
        "Answer in json format:\n",
        "\n",
        "\"answer\" : \"....\"\n",
        "\"relevent_law\" : \"...\"\n",
        "\n",
        ":\n",
        "\n",
        "###\n",
        "Chat history:\n",
        "{history}\n",
        "###\n",
        "\n",
        "\n",
        "'''\n",
        " {question}\n",
        "'''\n",
        "\n",
        " ```\n",
        " {context}\n",
        "  ```\n",
        "\n",
        " \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "PROMPT_TEMPLATE_ARTICLE = \\\n",
        "\"\"\"\n",
        "Please let me know if there are any articles in the following text.\n",
        "Only mention things that are affirmative. If the articles is mentioned but has an exclusionary nature, there is no need to list it.\n",
        "If question relating to decrees or resolutions, you should skip too.\n",
        "I have some example for you\n",
        "-------------------------------------\n",
        "\n",
        "\"Câu hỏi\" : \"Điều 12 là gì\"\n",
        "\n",
        "\"Trả lời\" : [\"12\"]\n",
        "\n",
        "\n",
        "\"Câu hỏi\": \"Điều 23 của luật doanh nghiệp việt nam là gì\"\n",
        "\n",
        "\"Trả lời\" : [\"23\"]\n",
        "\n",
        "\n",
        "\"Câu hỏi\" : \"Vấn đề này nói gì điều này\"\n",
        "\n",
        "\"Trả lời\" : \"Không\"\n",
        "\n",
        "\n",
        "\"Câu hỏi\" : \"Điều này là điều gì?\"\n",
        "\n",
        "\"Trả lời\" : \"Không\"\n",
        "\n",
        "\n",
        "\"Câu hỏi\" : \"Cô muốn nói điều gì với tôi?\"\n",
        "\n",
        "\"Trả lời\" : \"Không\"\n",
        "\n",
        "\n",
        "\"Câu hỏi\": \"Những điều khoản nào nói về việc thành lập doanh nghiêp\"\n",
        "\n",
        "\"Trả lời\" : \"Không\"\n",
        "\n",
        "\n",
        "\"Câu hỏi\": \"Điều 12 và điều 25 là gì?\"\n",
        "\n",
        "\"Trả lời\" : [\"12\", \"25\"]\n",
        "\n",
        "\n",
        "\"Câu hỏi\": \"Khoản 2 điều 17 là gì?\"\n",
        "\n",
        "\"Trả lời\" : [\"17\"]\n",
        "\n",
        "\n",
        "\"Câu hỏi\": \"Ngoài điều 13 còn điều nào nói về vấn đề này không?\"\n",
        "\n",
        "\"Trả lời\" : \"Không\"\n",
        "\n",
        "\n",
        "\"Câu hỏi\": \"Điều nào nói về luật thành lập doanh nghiệp, ngoại trừ điều 20\"\n",
        "\n",
        "\"Trả lời\" : \"Không\"\n",
        "\n",
        "\n",
        "\"Câu hỏi\": \"Phụ lục 1 nói về điều gì?\"\n",
        "\n",
        "\"Trả lời\" : \"Không\"\n",
        "\n",
        "\n",
        "\"Câu hỏi\": \"Nghị quyết 173 quy định điều gì?\"\n",
        "\n",
        "\"Trả lời\" : \"Không\"\n",
        "\n",
        "\"Câu hỏi\": \"Nghị quyết 135 là gì?\"\n",
        "\n",
        "\"Trả lời\" : \"Không\"\n",
        "\n",
        "-------------------------------------------------------\n",
        "\n",
        "Sau đây là câu hỏi của tôi :\n",
        "\n",
        "\"Câu hỏi\": {question}\n",
        "\n",
        "You should answer in json format with is key is \"articles\" and do not need to explain any else.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "law_dict = {\n",
        "    \"LDN\" : \"Luật Doanh Nghiệp\",\n",
        "    \"LLD\" : \"Luật Lao Động\",\n",
        "    \"LDT\" : \"Luật Đầu Tư\",\n",
        "    \"LCK\" : \"Luật Chứng Khoán\",\n",
        "    \"LTTNDN\" : \"Luật Thuế Thu Nhập Doanh Nghiệp\",\n",
        "    \"NQ173\" : \"Nghị Quyết 173\",\n",
        "    \"ND135\" : \"Nghị Định 135\",\n",
        "    \"ND145\" : \"Nghị Định 145\",\n",
        "}\n",
        "\n",
        "\n",
        "PROMPT_TEMPLATE_EVALUATION = \\\n",
        "\"\"\"\n",
        "Here is the question, context, answer and ground truth answer from the RAG model:\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Answer: {answer}\n",
        "\n",
        "Ground truth answer : {ground_truth}\n",
        "\n",
        "Please evaluate the answer on a scale from 1 to 100 based on the following criteria:\n",
        "\n",
        "Accuracy: Is the answer correct and relevant to the question and context?\n",
        "Completeness: Does the answer provide all the necessary information?\n",
        "Clarity: Is the answer clear, easy to understand, and well-structured?\n",
        "Naturalness: Does the answer sound natural and convincing, as if it were written by a human?\n",
        "\n",
        "\n",
        "\n",
        "Your answer should be in json format, and you don't need to reasoning, just give me the score\n",
        "\n",
        "Your answer should be like this:\n",
        "\n",
        "```json\n",
        "\"accuracy\" : 100\n",
        "\"completeness\" : 50\n",
        "\"clarity\" : 60\n",
        "\"naturalness\" : 20\n",
        "\"total\" : 58\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "db = None\n",
        "bm25_retriever = None\n",
        "reranker = None\n",
        "hybrid_search = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0B71qzdpF1GM",
        "outputId": "ba27295f-3f95-4b90-a571-7e148395d6fc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up dataset"
      ],
      "metadata": {
        "id": "QqE_YUIYD6sB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile setup.py\n",
        "\n",
        "import data_loader as dl\n",
        "import hybrid_search as hs\n",
        "import reranker as rk\n",
        "import config as cfg\n",
        "\n",
        "\n",
        "def setup():\n",
        "    # source_file = \"/content/drive/MyDrive/TDTU/project/LDN_UPDATED.zip\"\n",
        "\n",
        "    source_file = [\n",
        "        \"/content/drive/MyDrive/TDTU/project/source/zip/LDN_2020.zip\",\n",
        "        \"/content/drive/MyDrive/TDTU/project/source/zip/LDT_2020.zip\",\n",
        "        \"/content/drive/MyDrive/TDTU/project/source/zip/LCK_2019.zip\",\n",
        "        \"/content/drive/MyDrive/TDTU/project/source/zip/LLD_2019.zip\",\n",
        "        \"/content/drive/MyDrive/TDTU/project/source/zip/LTTNDN_2008.zip\",\n",
        "        \"/content/drive/MyDrive/TDTU/project/source/zip/NQ173_2024.zip\",\n",
        "        \"/content/drive/MyDrive/TDTU/project/source/zip/ND135_2020.zip\",\n",
        "        \"/content/drive/MyDrive/TDTU/project/source/zip/ND145_2020.zip\",\n",
        "    ]\n",
        "\n",
        "    destination_folder = '/content/books'\n",
        "    CHROMA_PATH = \"/content/chroma\"\n",
        "    chunk_size = 4096\n",
        "    chunk_overlap = 512\n",
        "\n",
        "    data_loader = dl.DataLoader(source_file, destination_folder, CHROMA_PATH, chunk_size, chunk_overlap)\n",
        "    db, bm25_retriever = data_loader.load_data()\n",
        "    reranker = rk.Reranker()\n",
        "    hybrid_search = hs.HybridSearch(db, bm25_retriever)\n",
        "\n",
        "    cfg.db = db\n",
        "    cfg.bm25_retriever = bm25_retriever\n",
        "    cfg.reranker = reranker\n",
        "    cfg.hybrid_search = hybrid_search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oakWkW3hCuJB",
        "outputId": "7e91a18a-81eb-45fa-d13e-428fadf555c0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import setup\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "setup.setup()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-xWIk6Vj9G2",
        "outputId": "9d389216-a0d4-4a1e-f695-0700a8f65c20",
        "collapsed": true
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully unzipped '/content/drive/MyDrive/TDTU/project/source/zip/LDN_2020.zip' to '/content/books'\n",
            "Successfully unzipped '/content/drive/MyDrive/TDTU/project/source/zip/LDT_2020.zip' to '/content/books'\n",
            "Successfully unzipped '/content/drive/MyDrive/TDTU/project/source/zip/LCK_2019.zip' to '/content/books'\n",
            "Successfully unzipped '/content/drive/MyDrive/TDTU/project/source/zip/LLD_2019.zip' to '/content/books'\n",
            "Successfully unzipped '/content/drive/MyDrive/TDTU/project/source/zip/LTTNDN_2008.zip' to '/content/books'\n",
            "Successfully unzipped '/content/drive/MyDrive/TDTU/project/source/zip/NQ173_2024.zip' to '/content/books'\n",
            "Successfully unzipped '/content/drive/MyDrive/TDTU/project/source/zip/ND135_2020.zip' to '/content/books'\n",
            "Successfully unzipped '/content/drive/MyDrive/TDTU/project/source/zip/ND145_2020.zip' to '/content/books'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 826/826 [00:15<00:00, 53.25it/s]\n",
            "/content/data_loader.py:51: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  db = Chroma.from_documents(chunks, embedding = HuggingFaceEmbeddings(model_name=\"keepitreal/vietnamese-sbert\"),\\\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/content/data_loader.py:54: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  db.persist()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Save 888 chunks to /content/chroma\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import streamlit as st\n",
        "# import utils_function as uf\n",
        "# import config as cfg\n",
        "# import setup\n",
        "# import time\n",
        "\n",
        "# db = cfg.db\n",
        "# bm25_retriever = cfg.bm25_retriever\n",
        "# reranker = cfg.reranker\n",
        "# hybrid_search = cfg.hybrid_search\n",
        "# PROMPT_TEMPLATE = cfg.PROMPT_TEMPLATE\n",
        "\n",
        "# prompt = \"tôi đang là bí thư xã, tôi có thể thành lập doanh nghiệp tư nhân được không?\"\n",
        "# response = uf.gen_ans(prompt, PROMPT_TEMPLATE, db, reranker, hybrid_search)\n",
        "\n"
      ],
      "metadata": {
        "id": "uoyIYioMG9hm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import utils_function as uf\n",
        "# hyDe =  uf.gen_hypothetical_document(prompt)"
      ],
      "metadata": {
        "id": "WfNrZi1FLp6M"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def wrap_text(text, width=70):\n",
        "  wrapped_text = textwrap.fill(text, width=width)\n",
        "  return wrapped_text\n"
      ],
      "metadata": {
        "id": "DuD7y6nXKilT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(wrap_text(hyDe.text))"
      ],
      "metadata": {
        "id": "LRcNppRmLuc8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(wrap_text(response[0]))"
      ],
      "metadata": {
        "id": "q10R_sNtKKwF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import utils_function as uf\n",
        "# import config as cfg\n",
        "\n",
        "\n",
        "# question = \"ngoài điều 13, còn điều nào quy định về trách nhiệm của người đại diện theo pháp luật của doanh nghiệp hay không?\"\n",
        "\n",
        "# response =  uf.classfication_article(question, cfg.PROMPT_TEMPLATE_ARTICLE)\n",
        "\n"
      ],
      "metadata": {
        "id": "QQ5PeFwwooWp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# response"
      ],
      "metadata": {
        "id": "q0MVd5WzY3eb"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## List of question for evaluation"
      ],
      "metadata": {
        "id": "8hhGTy_TNTb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# answer_list = []\n",
        "# context_list = []"
      ],
      "metadata": {
        "id": "MQukSqrZQSIs"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import utils_function as uf\n",
        "# import config as cfg\n",
        "# import time\n",
        "# import importlib\n",
        "\n",
        "# importlib.reload(uf)\n",
        "\n",
        "\n",
        "# promtp_template = cfg.PROMPT_TEMPLATE\n",
        "\n",
        "# db = cfg.db\n",
        "# reranker = cfg.reranker\n",
        "# hybrid_search = cfg.hybrid_search\n",
        "\n",
        "# data_path = \"/content/drive/MyDrive/TDTU/project/evaluation/question_rag.txt\"\n",
        "# question_list = uf.read_list_question(data_path)\n",
        "# question_list = question_list[:50]\n",
        "# print(len(question_list))\n",
        "\n",
        "\n",
        "# for q_index in range(len(question_list)):\n",
        "\n",
        "#   query = question_list[q_index]\n",
        "\n",
        "#   print(f\"\\n\\nQuestion {q_index}: {query}\")\n",
        "\n",
        "#   for i in range(3):\n",
        "\n",
        "#     response = uf.gen_ans(query, promtp_template, db, reranker, hybrid_search)\n",
        "\n",
        "#     print(response[0])\n",
        "#     time.sleep(20)\n",
        "\n",
        "\n",
        "\n",
        "#     if(response[0] == \"Đã có lỗi xảy ra, vui lòng thử lại\"):\n",
        "#       print(\"Đã có lỗi xảy ra, đang thử lại\")\n",
        "#       if(i == 2):\n",
        "#         answer_list.append(\"<<Không thể trả lời câu hỏi này>>\")\n",
        "#         context_list.append('<<'+response[3]+'>>')\n",
        "\n",
        "\n",
        "#     else:\n",
        "#       answer_list.append('<<'+response[0]+'>>')\n",
        "#       context_list.append('<<'+response[3]+'>>')\n",
        "#       break\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WZ9I4GHAJyn9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(answer_list)"
      ],
      "metadata": {
        "id": "_ZQM615e-S_t"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# answer_list = answer_list[:49]\n",
        "# context_list = context_list[:49]"
      ],
      "metadata": {
        "id": "QERNvvd-WUen"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(\"answer_list_noHyde_noHybrid.txt\", \"w\") as f:\n",
        "#   for answer in answer_list:\n",
        "#     f.write(answer + \"\\n\")\n",
        "\n",
        "# with open(\"context_list_noHyde_noHybrid.txt\", \"w\") as f:\n",
        "#   for answer in context_list:\n",
        "#     f.write(answer + \"\\n\")"
      ],
      "metadata": {
        "id": "ErdhZpm_W23F"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streamlit UI"
      ],
      "metadata": {
        "id": "QSoI_q2xHt_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Current UI"
      ],
      "metadata": {
        "id": "ECiowvTJo32q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Support function"
      ],
      "metadata": {
        "id": "LjF--TgAmCWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile support_function.py\n",
        "\n",
        "import streamlit as st\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "\n",
        "def new_chat(timestamp):\n",
        "\n",
        "    save_chat(timestamp)\n",
        "\n",
        "    st.session_state.messages = []\n",
        "    st.session_state.law = None\n",
        "    st.session_state.details_law = None\n",
        "\n",
        "    new_timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    st.session_state.current_chat = new_timestamp\n",
        "\n",
        "\n",
        "def get_title(messages, timestamp):\n",
        "    title_list = []\n",
        "    for msg in messages:\n",
        "        if msg[\"role\"] == \"user\":\n",
        "          if len(msg[\"content\"]) > 30:\n",
        "              title_list.append(msg[\"content\"][:30] + \"...\")\n",
        "          else:\n",
        "              title_list.append(msg[\"content\"])\n",
        "\n",
        "    if len(title_list) != 0:\n",
        "       return title_list[0]\n",
        "    else:\n",
        "       return timestamp\n",
        "\n",
        "\n",
        "def save_chat(timestamp):\n",
        "\n",
        "    if len(st.session_state.messages) <= 1:\n",
        "        return\n",
        "\n",
        "    chat_title = get_title(st.session_state.messages, timestamp)\n",
        "\n",
        "    st.session_state.history[timestamp] = {\n",
        "              \"messages\": st.session_state.messages.copy(),\n",
        "              \"law\": st.session_state.law,\n",
        "              \"details_law\": st.session_state.details_law,\n",
        "              \"title\": chat_title\n",
        "          }\n",
        "\n",
        "\n",
        "def load_chat(timestamp):\n",
        "    st.session_state.current_chat = timestamp\n",
        "    chat_data = st.session_state.history[timestamp]\n",
        "    st.session_state.messages = chat_data[\"messages\"]\n",
        "    st.session_state.law = chat_data[\"law\"]\n",
        "    st.session_state.details_law = chat_data[\"details_law\"]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-OxwA0UUmExR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "290d77e5-18ec-4603-cf4c-0ec9b2d6799c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting support_function.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main app"
      ],
      "metadata": {
        "id": "Ey_xTNQzmtJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import utils_function as uf\n",
        "import config as cfg\n",
        "import setup\n",
        "import support_function as sf\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "    <style>\n",
        "    .stButton>button {\n",
        "        width: 100%;\n",
        "    }\n",
        "    </style>\n",
        "    \"\"\",\n",
        "    unsafe_allow_html=True\n",
        ")\n",
        "\n",
        "\n",
        "if cfg.db == None:\n",
        "  setup.setup()\n",
        "\n",
        "db = cfg.db\n",
        "bm25_retriever = cfg.bm25_retriever\n",
        "reranker = cfg.reranker\n",
        "hybrid_search = cfg.hybrid_search\n",
        "PROMPT_TEMPLATE = cfg.PROMPT_TEMPLATE\n",
        "\n",
        "\n",
        "st.title(\"⚖️ Chatbot Tư Vấn Pháp Luật\")\n",
        "\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "if \"history\" not in st.session_state:\n",
        "    st.session_state.history = {}\n",
        "\n",
        "if \"current_chat\" not in st.session_state:\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    st.session_state.current_chat = timestamp\n",
        "\n",
        "\n",
        "if len(st.session_state.messages) == 0:\n",
        "    st.session_state.messages = [\n",
        "          {\"role\": \"assistant\", \"content\": \"Xin chào, tôi là chatbot tư vấn về Luật Doanh Nghiệp Việt Nam, tôi có thể giúp gì cho bạn?\"}\n",
        "      ]\n",
        "\n",
        "if \"law\" not in st.session_state:\n",
        "    st.session_state.law = None\n",
        "\n",
        "if \"details_law\" not in st.session_state:\n",
        "    st.session_state.details_law = None\n",
        "\n",
        "\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(f'<p style=\"text-align: justify;\">{message[\"content\"]}</p>', unsafe_allow_html=True)\n",
        "\n",
        "\n",
        "prompt = st.chat_input(\"Xin chào! Tôi có thể giúp gì cho bạn?\")\n",
        "\n",
        "if prompt:\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(f'<p style=\"text-align: justify;\">{prompt}</p>', unsafe_allow_html=True)\n",
        "\n",
        "\n",
        "    response = uf.gen_ans(prompt, PROMPT_TEMPLATE, db, reranker, hybrid_search, st.session_state.messages[:-1])\n",
        "\n",
        "    answer = response[0]\n",
        "    st.session_state.law = response[1]\n",
        "    st.session_state.details_law = response[2]\n",
        "\n",
        "\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer})\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.markdown(f'<p style=\"text-align: justify;\">{answer}</p>', unsafe_allow_html=True)\n",
        "\n",
        "\n",
        "\n",
        "# Sidebar\n",
        "with st.sidebar:\n",
        "\n",
        "    if len(st.session_state.messages) != 1 :\n",
        "      refesh_button = st.button(\"Đoạn chat mới\")\n",
        "\n",
        "      if refesh_button:\n",
        "        sf.new_chat(st.session_state.current_chat)\n",
        "\n",
        "\n",
        "\n",
        "    if st.session_state.law is not None:\n",
        "\n",
        "        if st.session_state.law != \"Không có luật liên quan\":\n",
        "            st.divider()\n",
        "            with st.expander(\"Luật liên quan\"):\n",
        "                st.markdown(f'<p style=\"text-align: justify;\">{st.session_state.law}</p>', unsafe_allow_html=True)\n",
        "\n",
        "            if st.session_state.details_law != \"Không có luật liên quan\":\n",
        "              st.markdown(f'<p style=\"text-align: justify;\">Văn bản ngữ cảnh</p>', unsafe_allow_html=True)\n",
        "\n",
        "              for article in st.session_state.details_law:\n",
        "                with st.expander(f\"{article[0]}\"):\n",
        "                    st.markdown(f'<p style=\"text-align: justify;\">{article[1]}</p>', unsafe_allow_html=True)\n",
        "\n",
        "\n",
        "    temp_histoy = st.session_state.history.copy()\n",
        "    current_chat = st.session_state.current_chat\n",
        "    if len(temp_histoy) != 0:\n",
        "        st.divider()\n",
        "        st.subheader(\"Previous Chats\")\n",
        "        for timestamp, chat_data in reversed(temp_histoy.items()):\n",
        "            if st.button(f\"{chat_data['title']}\", key=timestamp,type=\"secondary\" if timestamp != current_chat else \"primary\"\n",
        "            ):\n",
        "                sf.new_chat(current_chat)\n",
        "                sf.load_chat(timestamp)\n",
        "\n"
      ],
      "metadata": {
        "id": "rtNG2VGmQC1s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59a28ad9-f99e-4cdd-d946-86769a26f43a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run app"
      ],
      "metadata": {
        "id": "TAcHKCLuQTe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import os\n",
        "from pyngrok import ngrok, exception\n",
        "ngrok.set_auth_token(\"2qz0BRsjBzGMWxObp6tTneRz2AT_86iyNsQdpg3UCr1rqvjwX\")\n",
        "\n",
        "\n",
        "\n",
        "# os.system(\"pkill streamlit\")\n",
        "# st.cache_data.clear()\n",
        "# !rm -rf /root/.cache/pyngrok\n",
        "!streamlit run /content/app.py &>/content/logs.txt &\n",
        "\n",
        "\n",
        "# Specify tunnel options using a dictionary\n",
        "tunnel_config = {\n",
        "    \"addr\": 8501,  # Port to expose\n",
        "    \"proto\": \"http\",  # Protocol (http or tcp)\n",
        "    # Add other options as needed\n",
        "}\n",
        "public_url = ngrok.connect(**tunnel_config)\n",
        "print(\"Ngrok URL:\", public_url)"
      ],
      "metadata": {
        "id": "0vtj6Jo2zyls",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "208df4e1-0380-4f7b-83bd-d7312f1e4f9c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ngrok URL: NgrokTunnel: \"https://d54e-34-16-147-46.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "mC5REvcVFb0R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define class"
      ],
      "metadata": {
        "id": "WFIqKTO0GoYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from rouge_score import rouge_scorer\n",
        "import pandas as pd\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "class Evaluation:\n",
        "\n",
        "  def get_data(self, path):\n",
        "    data = \"\"\n",
        "    with open(path, \"r\") as f:\n",
        "      data = f.read()\n",
        "    return data\n",
        "\n",
        "  def remove_special_chars(self, text):\n",
        "    return re.sub(r'[^\\w\\s\\u4e00-\\u9fff\\uac00-\\ud7af\\u3000-\\u303f\\ufb00-\\ufb06\\ufb13-\\ufb17]+', '', text)\n",
        "\n",
        "  def data_loader(self, question_path, answer_path, context_path, truth_path):\n",
        "\n",
        "    question_data = self.get_data(question_path)\n",
        "    answer_data = self.get_data(answer_path)\n",
        "    context_data = self.get_data(context_path)\n",
        "    truth_data = self.get_data(truth_path)\n",
        "\n",
        "    answer_data = answer_data.replace(\"\\n\", \"\").replace(\">>\",\"\\n\").replace(\"<<\", \"\")\n",
        "    context_data = context_data.replace(\"\\n\", \"\").replace(\">>\",\"\\n\").replace(\"<<\", \"\")\n",
        "    truth_data = truth_data.replace(\"\\n\", \"\").replace(\">>\",\">>\\n\").replace(\"<<\", \"\")\n",
        "\n",
        "    questions = question_data.strip().split('\\n')\n",
        "    answers = answer_data.strip().split('\\n')\n",
        "    contexts = context_data.strip().split('\\n')\n",
        "    truths = truth_data.strip().split('\\n')\n",
        "\n",
        "    data = []\n",
        "    min_len = min(len(questions), len(answers), len(truths))\n",
        "    for i in range(min_len):\n",
        "      data.append({'question': questions[i], 'answer': answers[i], 'truth': truths[i]})\n",
        "\n",
        "    # Create the DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    df = df.map(self.remove_special_chars)\n",
        "\n",
        "    return [questions, answers, contexts, truths], df\n",
        "\n",
        "\n",
        "  def calculate_rouge(self, generated, reference, scorer):\n",
        "    scores = scorer.score(reference, generated)\n",
        "    return {\n",
        "        \"rouge1\": scores['rouge1'].fmeasure,\n",
        "        \"rouge2\": scores['rouge2'].fmeasure,\n",
        "        \"rougeL\": scores['rougeL'].fmeasure\n",
        "    }\n",
        "\n",
        "  def calculate_bleu(self, generated, reference):\n",
        "    reference = [reference.split()]  # Reference should be a list of lists of tokens\n",
        "    generated = generated.split()\n",
        "    return sentence_bleu(reference, generated)\n",
        "\n",
        "  def evaluation(self, df):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    df['rouge_scores'] = df.apply(\n",
        "      lambda row: self.calculate_rouge(row['answer'], row['truth'], scorer),\n",
        "      axis=1)\n",
        "\n",
        "    df['rouge1'] = df['rouge_scores'].apply(lambda x: x['rouge1'])\n",
        "    df['rouge2'] = df['rouge_scores'].apply(lambda x: x['rouge2'])\n",
        "    df['rougeL'] = df['rouge_scores'].apply(lambda x: x['rougeL'])\n",
        "\n",
        "    average_rouge1 = df['rouge1'].mean()\n",
        "    average_rouge2 = df['rouge2'].mean()\n",
        "    average_rougeL = df['rougeL'].mean()\n",
        "\n",
        "    print(f\"Average ROUGE-1: {average_rouge1:.4f}\")\n",
        "    print(f\"Average ROUGE-2: {average_rouge2:.4f}\")\n",
        "    print(f\"Average ROUGE-L: {average_rougeL:.4f}\")\n",
        "\n",
        "    df['bleu_score'] = df.apply(\n",
        "        lambda row: self.calculate_bleu(row['answer'], row['truth']),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    average_bleu = df['bleu_score'].mean()\n",
        "    print(f\"\\n\\nAverage BLEU Score: {average_bleu:.4f}\")\n"
      ],
      "metadata": {
        "id": "L0xu60sm4IsB"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Without HyDE"
      ],
      "metadata": {
        "id": "vlLOlp_YG8fi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = Evaluation()"
      ],
      "metadata": {
        "id": "NN2u7kwMHI1p"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_path = \"/content/drive/MyDrive/TDTU/project/evaluation/question_rag.txt\"\n",
        "answer_path = \"/content/drive/MyDrive/TDTU/project/evaluation/answer_list.txt\"\n",
        "context_path = \"/content/drive/MyDrive/TDTU/project/evaluation/context_list.txt\"\n",
        "truth_path = \"/content/drive/MyDrive/TDTU/project/evaluation/truth_answer.txt\""
      ],
      "metadata": {
        "id": "crZ90rSvHB5B"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_data, df = evaluator.data_loader(question_path, answer_path, context_path, truth_path)\n",
        "\n",
        "evaluator.evaluation(df)"
      ],
      "metadata": {
        "id": "JJmlJ3nvHGJB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8409a992-6f5e-4c28-dfae-15d3bcffb8b2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average ROUGE-1: 0.7347\n",
            "Average ROUGE-2: 0.6568\n",
            "Average ROUGE-L: 0.6657\n",
            "\n",
            "\n",
            "Average BLEU Score: 0.4546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With HyDe"
      ],
      "metadata": {
        "id": "pIcIkQgpHhCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer_path = \"/content/drive/MyDrive/TDTU/project/evaluation/answer_list_hyDe_version_6.txt\"\n",
        "context_path = \"/content/drive/MyDrive/TDTU/project/evaluation/context_list_hyDe_version_6.txt\""
      ],
      "metadata": {
        "id": "ILLLCtpSHjPx"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_data_HyDE, df_HyDE = evaluator.data_loader(question_path, answer_path, context_path, truth_path)\n",
        "\n",
        "evaluator.evaluation(df_HyDE)"
      ],
      "metadata": {
        "id": "q0KJhkksHf4B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f87cb79-39ab-428d-8aef-cf84765aa3d4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average ROUGE-1: 0.7186\n",
            "Average ROUGE-2: 0.6195\n",
            "Average ROUGE-L: 0.6200\n",
            "\n",
            "\n",
            "Average BLEU Score: 0.4609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With Hyde , No Hybrid"
      ],
      "metadata": {
        "id": "2pOyLW1xzo7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer_path = \"/content/drive/MyDrive/TDTU/project/evaluation/answer_list_hyde_noHybrid.txt\"\n",
        "context_path = \"/content/drive/MyDrive/TDTU/project/evaluation/context_list_hyde_noHybrid.txt\""
      ],
      "metadata": {
        "id": "khzu-ihRzoiZ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_data_HyDE_noHybrid, df_HyDE_noHybrid = evaluator.data_loader(question_path, answer_path, context_path, truth_path)\n",
        "\n",
        "evaluator.evaluation(df_HyDE_noHybrid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYrYoG9L2PEt",
        "outputId": "8d49f386-8cca-4f04-fb7b-ee2952db15bf"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average ROUGE-1: 0.7558\n",
            "Average ROUGE-2: 0.6311\n",
            "Average ROUGE-L: 0.6379\n",
            "\n",
            "\n",
            "Average BLEU Score: 0.4568\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_path = \"/content/drive/MyDrive/TDTU/project/evaluation/answer_list_noHyde_noHybrid.txt\"\n",
        "context_path = \"/content/drive/MyDrive/TDTU/project/evaluation/context_list_noHyde_noHybrid.txt\""
      ],
      "metadata": {
        "id": "SwVv3SHp-v0G"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_data_noHyDE_noHybrid, df_noHyDE_noHybrid = evaluator.data_loader(question_path, answer_path, context_path, truth_path)\n",
        "\n",
        "evaluator.evaluation(df_noHyDE_noHybrid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efINj8dK-zfs",
        "outputId": "e00a2929-3dd8-49a5-f9a9-b21e81353740"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average ROUGE-1: 0.7756\n",
            "Average ROUGE-2: 0.6643\n",
            "Average ROUGE-L: 0.6692\n",
            "\n",
            "\n",
            "Average BLEU Score: 0.4948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import utils_function as uf\n",
        "import config as cfg\n",
        "import importlib\n",
        "import time\n",
        "\n",
        "importlib.reload(uf)\n",
        "\n",
        "\n",
        "total = []\n",
        "\n",
        "for i in range(len(evaluation_data[0])):\n",
        "\n",
        "    question = evaluation_data[0][i]\n",
        "    context = evaluation_data[2][i]\n",
        "    answer = evaluation_data[1][i]\n",
        "    ground_truth = evaluation_data[3][i]\n",
        "\n",
        "    response = uf.gen_evaluation_score(cfg.PROMPT_TEMPLATE_EVALUATION,question, context, answer, ground_truth)\n",
        "\n",
        "    print(f\"{i}: {response}\")\n",
        "\n",
        "    time.sleep(6)\n",
        "\n",
        "\n",
        "\n",
        "    total.append(response['total'])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        },
        "id": "CllHd_v3slWe",
        "outputId": "91504f7d-5b17-4db8-b1af-21708e02cf2e"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: {'accuracy': 100, 'completeness': 50, 'clarity': 90, 'naturalness': 80, 'total': 80}\n",
            "1: {'accuracy': 90, 'completeness': 60, 'clarity': 90, 'naturalness': 80, 'total': 80}\n",
            "2: {'accuracy': 100, 'completeness': 100, 'clarity': 100, 'naturalness': 100, 'total': 100}\n",
            "3: {'accuracy': 100, 'completeness': 100, 'clarity': 100, 'naturalness': 100, 'total': 100}\n",
            "4: {'accuracy': 70, 'completeness': 40, 'clarity': 80, 'naturalness': 70, 'total': 65}\n",
            "5: {'accuracy': 100, 'completeness': 100, 'clarity': 90, 'naturalness': 90, 'total': 95}\n",
            "6: {'accuracy': 100, 'completeness': 100, 'clarity': 100, 'naturalness': 95, 'total': 99}\n",
            "7: {'accuracy': 100, 'completeness': 80, 'clarity': 90, 'naturalness': 90, 'total': 90}\n",
            "8: {'accuracy': 100, 'completeness': 100, 'clarity': 100, 'naturalness': 100, 'total': 100}\n",
            "9: {'accuracy': 100, 'completeness': 100, 'clarity': 100, 'naturalness': 95, 'total': 99}\n",
            "10: {'accuracy': 100, 'completeness': 70, 'clarity': 80, 'naturalness': 90, 'total': 85}\n",
            "11: {'accuracy': 100, 'completeness': 100, 'clarity': 100, 'naturalness': 100, 'total': 100}\n",
            "12: {'accuracy': 100, 'completeness': 100, 'clarity': 100, 'naturalness': 100, 'total': 100}\n",
            "13: {'accuracy': 100, 'completeness': 80, 'clarity': 100, 'naturalness': 100, 'total': 95}\n",
            "14: {'accuracy': 100, 'completeness': 100, 'clarity': 100, 'naturalness': 100, 'total': 100}\n",
            "15: {'accuracy': 100, 'completeness': 80, 'clarity': 90, 'naturalness': 90, 'total': 90}\n",
            "16: {'accuracy': 100, 'completeness': 100, 'clarity': 100, 'naturalness': 100, 'total': 100}\n",
            "17: {'accuracy': 100, 'completeness': 100, 'clarity': 100, 'naturalness': 100, 'total': 100}\n",
            "18: {'accuracy': 100, 'completeness': 80, 'clarity': 90, 'naturalness': 90, 'total': 90}\n",
            "19: {'accuracy': 100, 'completeness': 70, 'clarity': 70, 'naturalness': 60, 'total': 75}\n",
            "20: {'accuracy': 100, 'completeness': 70, 'clarity': 80, 'naturalness': 80, 'total': 83}\n",
            "21: {'accuracy': 100, 'completeness': 100, 'clarity': 100, 'naturalness': 100, 'total': 100}\n",
            "22: {'accuracy': 100, 'completeness': 100, 'clarity': 100, 'naturalness': 100, 'total': 100}\n",
            "23: {'accuracy': 100, 'completeness': 80, 'clarity': 90, 'naturalness': 90, 'total': 90}\n",
            "24: {'accuracy': 100, 'completeness': 100, 'clarity': 100, 'naturalness': 100, 'total': 100}\n",
            "25: {'accuracy': 100, 'completeness': 80, 'clarity': 90, 'naturalness': 90, 'total': 90}\n",
            "26: {'accuracy': 100, 'completeness': 70, 'clarity': 90, 'naturalness': 80, 'total': 85}\n",
            "27: {'accuracy': 100, 'completeness': 70, 'clarity': 80, 'naturalness': 70, 'total': 80}\n",
            "28: {'accuracy': 100, 'completeness': 80, 'clarity': 90, 'naturalness': 90, 'total': 90}\n",
            "29: {'accuracy': 100, 'completeness': 80, 'clarity': 90, 'naturalness': 80, 'total': 88}\n",
            "30: {'accuracy': 100, 'completeness': 60, 'clarity': 90, 'naturalness': 90, 'total': 85}\n",
            "31: {'accuracy': 100, 'completeness': 80, 'clarity': 90, 'naturalness': 90, 'total': 90}\n",
            "32: {'accuracy': 0, 'completeness': 0, 'clarity': 100, 'naturalness': 100, 'total': 25}\n",
            "33: {'accuracy': 100, 'completeness': 70, 'clarity': 90, 'naturalness': 80, 'total': 85}\n",
            "34: {'accuracy': 100, 'completeness': 80, 'clarity': 90, 'naturalness': 90, 'total': 90}\n",
            "35: {'accuracy': 100, 'completeness': 80, 'clarity': 90, 'naturalness': 80, 'total': 87}\n",
            "36: {'accuracy': 100, 'completeness': 100, 'clarity': 100, 'naturalness': 100, 'total': 100}\n",
            "37: {'accuracy': 100, 'completeness': 70, 'clarity': 90, 'naturalness': 90, 'total': 87}\n",
            "38: {'accuracy': 100, 'completeness': 100, 'clarity': 100, 'naturalness': 100, 'total': 100}\n",
            "39: {'accuracy': 100, 'completeness': 70, 'clarity': 90, 'naturalness': 80, 'total': 85}\n",
            "40: {'accuracy': 50, 'completeness': 20, 'clarity': 80, 'naturalness': 80, 'total': 57}\n",
            "41: {'accuracy': 100, 'completeness': 70, 'clarity': 90, 'naturalness': 80, 'total': 85}\n",
            "42: {'accuracy': 100, 'completeness': 70, 'clarity': 90, 'naturalness': 80, 'total': 85}\n",
            "43: {'accuracy': 100, 'completeness': 70, 'clarity': 90, 'naturalness': 80, 'total': 85}\n",
            "44: {'accuracy': 100, 'completeness': 50, 'clarity': 100, 'naturalness': 100, 'total': 87}\n",
            "45: {'accuracy': 100, 'completeness': 70, 'clarity': 100, 'naturalness': 100, 'total': 92}\n",
            "46: {'accuracy': 100, 'completeness': 90, 'clarity': 95, 'naturalness': 85, 'total': 93}\n",
            "47: {'accuracy': 100, 'completeness': 100, 'clarity': 100, 'naturalness': 100, 'total': 100}\n",
            "48: {'accuracy': 100, 'completeness': 80, 'clarity': 90, 'naturalness': 90, 'total': 90}\n",
            "49: {'accuracy': 100, 'completeness': 80, 'clarity': 90, 'naturalness': 80, 'total': 88}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum = 0\n",
        "for i in range(len(total)):\n",
        "    sum += total[i]\n",
        "\n",
        "print(sum/len(total))"
      ],
      "metadata": {
        "id": "Hk53BiaP3dwl",
        "outputId": "75ccb40b-ffdb-4f32-967f-a9abbd22b2d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88.9\n"
          ]
        }
      ]
    }
  ]
}